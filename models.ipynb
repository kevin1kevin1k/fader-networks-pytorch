{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_BN_LR(c_in, c_out, activation, transpose=False, dropout=None):\n",
    "    layers = []\n",
    "    if not transpose:\n",
    "        layers.append(         nn.Conv2d(c_in, c_out, kernel_size=4, stride=2, padding=1))\n",
    "    else:\n",
    "        layers.append(nn.ConvTranspose2d(c_in, c_out, kernel_size=4, stride=2, padding=1))\n",
    "    if dropout:\n",
    "        layers.append(nn.Dropout2d(dropout))\n",
    "    layers.append(nn.BatchNorm2d(c_out))\n",
    "    layers.append(activation)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_channels = 3\n",
    "image_size = 64 # or 256\n",
    "if image_size == 64:\n",
    "    k_list = [num_channels, 16, 32, 64, 128, 256, 512] #, 512]\n",
    "else:\n",
    "    k_list = [num_channels, 16, 32, 64, 128, 256, 512, 512]\n",
    "batch_size = 32\n",
    "num_attr = 23\n",
    "attr_dim = 23\n",
    "# note that according to the paper, attr_dim = num_attr * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Input: (batch_size, num_channels, H, W)\n",
    "    Output: (batch_size, 512, H / 2**7, W / 2**7)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        activation = nn.LeakyReLU(0.2)\n",
    "        layers = []\n",
    "        for i in range(1, len(k_list)):\n",
    "            c_in, c_out = k_list[i - 1], k_list[i]\n",
    "            layers.append(C_BN_LR(c_in, c_out, activation))\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Ex = self.convs(x)\n",
    "        return Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Concat_C_BN_LR(nn.Module):\n",
    "#     '''\n",
    "#     Input: (batch_size, c_in, H, W), (batch_size, attr_dim)\n",
    "#     Output: (batch_size, c_out + attr_dim, H, W)\n",
    "#     '''\n",
    "#     def __init__(self, c_in, c_out, activation, transpose=True):\n",
    "#         super(Concat_C_BN_LR, self).__init__()\n",
    "#         self.conv = C_BN_LR(c_in, c_out, activation, transpose)\n",
    "\n",
    "#     def forward(self, x, attrs):\n",
    "#         H, W = x.size()[2], x.size()[3]\n",
    "#         attrs_ = attrs.repeat(H, W, 1, 1).permute(2, 3, 0, 1)\n",
    "#         x = torch.cat([x, attrs_], dim=1)\n",
    "#         x = self.conv(x)\n",
    "#         return x, attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Input: (batch_size, 512, H, W), (batch_size, attr_dim)\n",
    "    Output: (batch_size, 3, H * 2**7, W * 2**7)\n",
    "    '''\n",
    "    def __init__(self, attr_dim, image_size=256, num_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        activation = nn.ReLU()\n",
    "#         layers = []\n",
    "#         for i in range(len(k_list) - 1, 0, -1):\n",
    "#             c_in, c_out = k_list[i] + attr_dim, k_list[i - 1]\n",
    "#             layers.append(Concat_C_BN_LR(c_in, c_out, activation, transpose=True))\n",
    "#         self.deconvs = nn.Sequential(*layers)\n",
    "        \n",
    "        self.image_size = image_size\n",
    "        if self.image_size == 256:\n",
    "            self.deconv1 = C_BN_LR(k_list[7] + attr_dim, k_list[6], activation, transpose=True)\n",
    "        self.deconv2 = C_BN_LR(k_list[6] + attr_dim, k_list[5], activation, transpose=True)\n",
    "        self.deconv3 = C_BN_LR(k_list[5] + attr_dim, k_list[4], activation, transpose=True)\n",
    "        self.deconv4 = C_BN_LR(k_list[4] + attr_dim, k_list[3], activation, transpose=True)\n",
    "        self.deconv5 = C_BN_LR(k_list[3] + attr_dim, k_list[2], activation, transpose=True)\n",
    "        self.deconv6 = C_BN_LR(k_list[2] + attr_dim, k_list[1], activation, transpose=True)\n",
    "        self.deconv7 = C_BN_LR(k_list[1] + attr_dim, k_list[0], nn.Tanh(), transpose=True)\n",
    "        \n",
    "    def repeat_concat(self, Ex, attrs):\n",
    "        H, W = Ex.size()[2], Ex.size()[3]\n",
    "        attrs_ = attrs.repeat(H, W, 1, 1).permute(2, 3, 0, 1)\n",
    "        Ex_ = torch.cat([Ex, attrs_], dim=1)\n",
    "        return Ex_\n",
    "        \n",
    "    def forward(self, Ex, attrs):\n",
    "        if self.image_size == 256:\n",
    "            Ex = self.deconv1(self.repeat_concat(Ex, attrs))\n",
    "        Ex = self.deconv2(self.repeat_concat(Ex, attrs))\n",
    "        Ex = self.deconv3(self.repeat_concat(Ex, attrs))\n",
    "        Ex = self.deconv4(self.repeat_concat(Ex, attrs))\n",
    "        Ex = self.deconv5(self.repeat_concat(Ex, attrs))\n",
    "        Ex = self.deconv6(self.repeat_concat(Ex, attrs))\n",
    "        Ex = self.deconv7(self.repeat_concat(Ex, attrs))\n",
    "        return Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Input: (batch_size, 512, H / 2**7, W / 2**7)\n",
    "    Output: (batch_size, num_attrs)\n",
    "    '''\n",
    "    def __init__(self, num_attrs, image_size=256):\n",
    "        super(Discriminator, self).__init__()\n",
    "        if image_size == 256:\n",
    "            self.conv = C_BN_LR(512, 512, nn.LeakyReLU(0.2)) # ReLU? Dropout?\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.dp1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(512, num_attrs)\n",
    "        self.dp2 = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, Ex):\n",
    "        if image_size == 256:\n",
    "            Ex = self.conv(Ex)\n",
    "        p = Ex.view(Ex.size()[0], Ex.size()[1])\n",
    "        p = self.dp1(self.fc1(p))\n",
    "        p = self.dp2(self.fc2(p))\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    Enc = Encoder()\n",
    "    x = Variable(torch.zeros(32, 3, image_size, image_size))\n",
    "    Ex = Enc(x)\n",
    "    print(Ex.size())\n",
    "\n",
    "    Dis = Discriminator(10)\n",
    "    p = Dis(Ex)\n",
    "    print(p.size())\n",
    "\n",
    "    Dec = Decoder(20, image_size=image_size)\n",
    "    attrs = Variable(torch.zeros(32, 20))\n",
    "    x_ = Dec(Ex, attrs)\n",
    "    print(x_.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
